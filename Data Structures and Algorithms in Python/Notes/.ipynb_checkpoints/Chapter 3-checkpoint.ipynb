{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0603219c",
   "metadata": {},
   "source": [
    "# Experimental Studies\n",
    "\n",
    "**Challenges of Experimental Analysis**\n",
    "\n",
    "There are three major limitations to their use for\n",
    "algorithm analysis:\n",
    "\n",
    "* Experimental running times of two algorithms are difficult to directly compare unless the experiments are performed in the same hardware and software environments.\n",
    "\n",
    "* Experiments can be done only on a limited set of test inputs; hence, they leave out the running times of inputs not included in the experiment (and these inputs may be important).\n",
    "\n",
    "* An algorithm must be fully implemented to execute it to study its running time experimentally.\n",
    "\n",
    "## Moving Beyond Experimental Analysis\n",
    "Our goal is to develop an approach to analyzing the efficiency of algorithms that:\n",
    "\n",
    "1. Allows us to evaluate the relative efficiency of any two algorithms in a way that is independent of the hardware and software environment.\n",
    "\n",
    "2. Is performed by studying a high-level description of the algorithm without the need for implementation.\n",
    "\n",
    "3. Takes into account all possible inputs\n",
    "\n",
    "**Primitive Operations**\n",
    "\n",
    "* Formally, a primitive operation corresponds to a low-level instruction with an execution time that is constant. Ideally, this might be the type of basic operation that is executed by the hardware, although many of our primitive operations may be translated to a small number of instructions.\n",
    "* The implicit assumption in the approach of counting primitive operations as an indication of the running time is that the running times of different primitive operations will be fairly similar. Thus, the number, t, of primitive operations an algorithm performs will be proportional to the actual running time of that algorithm.\n",
    "\n",
    "**Focusing on the Worst-Case Input**\n",
    "\n",
    "Making the standard of success for an algorithm to perform well in the worst case necessarily requires that it will do well on every input.\n",
    "\n",
    "\n",
    "# The Seven Functions Used in The Book\n",
    "\n",
    "## The Constant Function\n",
    "\\begin{gather*}\n",
    "f (n) = c,\n",
    "\\end{gather*}\n",
    "\n",
    "* It does not matter what the value of n is; f (n) will always be equal to the constant value c.\n",
    "\n",
    "* As simple as it is, the constant function is useful in algorithm analysis, because it characterizes the number of steps needed to do a basic operation on a computer, like adding two numbers, assigning a value to some variable, or comparing two numbers.\n",
    "\n",
    "## The Logarithm Function\n",
    "\\begin{gather*}\n",
    "x = {log}_{b}n\\ if\\ and\\ only\\ if\\ b^x = n.\n",
    "\\end{gather*}\n",
    "\n",
    "* The most common base for the logarithm function in computer science is 2, as computers store integers in binary, and because a common operation in many algorithms is to repeatedly divide an input in half.\n",
    "\n",
    "* In particular, we can easily compute the smallest integer greater than or equal to ${log}_{b}n$ (its so-called ceiling, $\\lceil {log}_{b}n \\rceil$). For positive integer, his value is equal to the number of times we can divide n by b before we geta number less than or equal to 1. For example, the evaluation of ${log}_{3}27$ is 3, because $((27/3)/3)/3 = 1$. Likewise, ${log}_{4}64$ is 3, because $((64/4)/4)/4$ = 1, and ${log}_{2}12$ is 4, because $(((12/2)/2)/2)/2 = 0.75 \\le 1$.\n",
    "\n",
    "**Proposition 3.1 (Logarithm Rules)**: $Given\\ real\\ numbers\\ a > 0, b > 1, c > 0,$ and $d > 1$, we have:\n",
    "\n",
    "* ${log}_{b}(ac) = {log}_{b}a + {log}_{b}c$\n",
    "* ${log}_{b}(a/c) = {log}_{b}a - {log}_{b}c$\n",
    "* ${log}_{b}(a^c) = c\\ {log}_{b}a$\n",
    "* ${log}_{b}a = \\frac{{log}_{d}a}{{log}_{d}b}$\n",
    "* ${b}^{{log}_{d}a} = {a}^{{log}_{d}b}$\n",
    "\n",
    "\n",
    "## The Linear Function\n",
    "\\begin{gather*}\n",
    "f (n) = n.\n",
    "\\end{gather*}\n",
    "\n",
    "* This function arises in algorithm analysis any time we have to do a single basic operation for each of $n$ elements.\n",
    "* The linear function also represents the best running time we can hope to achieve for any algorithm that processes each of n objects that are not already in the computer’s memory because reading in the n objects already requires n operations.\n",
    "\n",
    "\n",
    "## The N-Log-N Function\n",
    "\\begin{gather*}\n",
    "f (n) = n\\ logn,\n",
    "\\end{gather*}\n",
    "\n",
    "* This function grows a little more rapidly than the linear function and a lot less rapidly than the quadratic function; therefore, we would greatly prefer an algorithm with a running time that is proportional to $nlogn$, than one with quadratic running time.\n",
    "\n",
    "\n",
    "## The Quadratic Function\n",
    "\\begin{gather*}\n",
    "f (n) = n^2,\n",
    "\\end{gather*}\n",
    "\n",
    "* The main reason why the quadratic function appears in the analysis of algorithms is that many algorithms have nested loops, where the inner loop performs a linear number of operations and the outer loop is performed a linear number of times. Thus, in such cases, the algorithm performs $n · n = {n}^{2}$ operations.\n",
    "\n",
    "* The quadratic function can also arise in the context of nested loops where the number of operations performed inside the loop increases by one with each iteration of the outer loop.\n",
    "\n",
    "## The Cubic Function and Other Polynomials\n",
    "A polynomial function has the form,\n",
    "\\begin{gather*}\n",
    "f(n) = a_0 +a_1n+a_2n^2 +a_3n^3 +···+a_dn^d,\n",
    "\\end{gather*}\n",
    "where $a_0,a_1,...,a_d$ are constants, called the coefficients of the polynomial, and $a_d \\neq 0$. Integer $d$, which indicates the highest power in the polynomial, is called the degree of the polynomial.\n",
    "\n",
    "## The Exponential Function\n",
    "\\begin{gather*}\n",
    "f(n) = b^n,\n",
    "\\end{gather*}\n",
    "\n",
    "* As was the case with the logarithm function, the most common base for the exponential function in algorithm analysis is $b = 2$.\n",
    "\n",
    "**Proposition 3.4 (Exponent Rules):** $Given\\ positive\\ integers\\ a,\\ b,\\ and\\ c,\\ we\\ have:$\n",
    "\n",
    "1. $(b^a)^c = b^{ac}$\n",
    "\n",
    "2. $b^ab^c = b^{a+c}$\n",
    "\n",
    "3. $b^a/b^c = b^{a−c}$\n",
    "\n",
    "* Given a positive integer k, we define $b^{1/k}$ to be $k^{th}$ root of $b$, that is, the number $r$ such that $r^k = b$. For example,\n",
    "$25^{1/2} = 5$, since $5^2 = 25$. Likewise, $27^{1/3} = 3$ and $16^{1/4} = 2$.\n",
    "\n",
    "* $b^{a/c}$ is really just the $c^{th}$ root of the integral exponent $b^a$.\n",
    "\n",
    "* Given a negative exponent $d$, we define $b^d = 1/b^{−d}$, which corresponds to applying Exponent Rule 3 with $a = 0$ and $c = −d$. For example, $2^{−3} = 1/2^3 = 1/8$.\n",
    "\n",
    "\n",
    "**Geometric Sums**\n",
    "\n",
    "For any integer $n ≥ 0$ and any real number a such that $a > 0$ and $a \\neq 1$, consider the summation:\n",
    "\\begin{gather*}\n",
    "\\sum_{i=0}^{n} a^i = 1 + a + a^2 + ··· + a^n\n",
    "\\end{gather*}\n",
    "\n",
    "(remembering that $a_0= 1$ if $a > 0$). This summation is equal to:\n",
    "\\begin{gather*}\n",
    "\\frac{a^{n+1} − 1}{a − 1}.\n",
    "\\end{gather*}\n",
    "\n",
    "***\n",
    "\n",
    "## Comparing Growth Rates\n",
    "\n",
    "Ideally, we would like data structure operations to run in times proportional to the constant or logarithm function, and we would like our algorithms to run in linear or n-log-n time. Algorithms with quadratic or cubic running times are less practical, and algorithms with exponential running times are infeasible for all but the smallest sized inputs.\n",
    "\n",
    "### The Ceiling and Floor Functions\n",
    "\n",
    "The analysis of an algorithm may sometimes involve the use of the floor function and ceiling function, which are defined respectively as follows:\n",
    "\n",
    "* $\\lfloor x \\rfloor$ = the largest integer less than or equal to $x$.\n",
    "* $\\lceil x \\rceil$ = the smallest integer greater than or equal to $x$.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "# Asymptotic Analysis\n",
    "We analyze algorithms using a mathematical notation for functions that disregards constant factors. Namely, we characterize the running times of algorithms by using functions that map the size of the input, n, to values that correspond to the main factor that determines the growth rate in terms of n. This approach reflects that each basic step in a pseudo-code description or a high-level language implementation may correspond to a small number of primitive operations. Thus, we can perform an analysis of an algorithm by estimating the number of primitive operations executed up to a constant factor, rather than getting bogged down in language-specific or hardware-specific analysis of the exact number of operations that execute on the computer.\n",
    "\n",
    "## The “Big-Oh” Notation\n",
    "Let $f(n)$ and $g(n)$ be functions mapping positive integers to positive real numbers.\n",
    "We say that $f(n)$ is $O(g(n))$ if there is a real constant $c > 0$ and an integer $n_0 ≥ 1$ such that:\n",
    "\\begin{gather*}\n",
    "f (n) \\le cg(n),\\ for\\ n \\ge n_0.\n",
    "\\end{gather*}\n",
    "\n",
    "* The Big-Oh notation allows us to say that a function $f(n)$ is “less than or equal her function $g(n)$ up to a constant factor and in the asymptotic sense as $n$ grows toward infinity. This ability comes from the fact that the definition uses “$\\le$” to compare $f(n)$ to a $g(n)$ times a constant, $c$, for the asymptotic cases when $n \\ge n_0$.\n",
    "\n",
    "* It is considered poor taste to say “$f(n) \\le O(g(n)),$” since the Big-Oh already denotes the “less-than-or-equal-to” concept. Likewise, although common, it is not fully correct to say “$f(n) = O(g(n)),$” with the usual understanding of the “=” relation, because there is no way to make sense of the symmetric statement, “$O(g(n)) = f(n)$”. It is best to say,“$f(n)\\ is\\ O(g(n))$.”\n",
    "\n",
    "* We can say “$f(n)$ is order of $g(n)$.” For the more mathematically $f(n) \\in O(g(n))y$, it is also correct to say, “$f(n) \\in O(g(n))$,” for the Big-Oh notation, technilly speaking, denotes a whole collection of functions.\n",
    "\n",
    "**Characterizing Running Times Using the Big-Oh Notation**\n",
    "\n",
    "The Big-Oh notation is used widely to characterize running times and space bounds in terms of some parameter n, which varies from problem to problem, but is always defined as a chosen measure of the “size” of the problem.\n",
    "\n",
    "**Some Properties of the Big-Oh Notation**\n",
    "\n",
    "The big-Oh notation allows us to ignore constant factors and lower-order terms and focus on the main components of a function that affect its growth.\n",
    "\n",
    "**Proposition 3.9**: If $f(n)$ is a polynomial of degree $d$, that is,\n",
    "\\begin{gather*}\n",
    "f (n) = a_0 + a_1n + ··· + a_dn^d,\n",
    "\\end{gather*}\n",
    "and $a_d > 0$, then $f(n)$ is $O(n^d)$.\n",
    "Justification:Note that, for $n ≥ 1$, we have $1 ≤ n ≤ n^2 ≤ ··· ≤ n^d$; hence,\n",
    "$a_0 + a_1n + a_2n^2 + ··· + a_dn^d ≤ (|a_0| + |a_1| + |a_2| + ···+ |a_d|)n^d.$\n",
    "We show that $f(n)$ is $O(n^d)$ by defining $c = |a_0| + |a_1| + ···+ |a_d|$ and $n_0 = 1$.\n",
    "\n",
    "Thus, the highest-degree term in a polynomial is the term that determines the asymptotic growth rate of that polynomial.\n",
    "\n",
    "**Characterizing Functions in Simplest Terms**\n",
    "\n",
    "* In general, we should use the big-Oh notation to characterize a function as closely as possible. While it is true that the function $f(n) = 4n^3 + 3n^2$ is $O(n^5)$ or even $O(n^4)$, it is more accurate to say that $f(n)$ is $O(n^3)$\n",
    "\n",
    "* It is also considered poor taste to include constant factors and lower-order terms in the big-Oh notation. For example, it is not fashionable to say that the function $2n^2$ is $O(4n^2 + 6n\\ logn)$, although this is completely correct. We should strive instead to describe the function in the Big-Oh in simple instead to describe the function in the Big-Oh in simplest terms.\n",
    "\n",
    "* Indeed, we typically use the names of these functions to refer to the running times of the algorithms they characterize. So, for example, we would say that an algorithm that runs in worst-case time $4n^2 +n\\ logn$ is a quadratic-time algorithm since it runs in $O(n2)$ time. Likewise, an algorithm running in at most $5n+20logn+4$ would be called a linear-time algorithm.\n",
    "\n",
    "### Big-Omega\n",
    "\n",
    "Let $f(n)$ and $g(n)$ be functions mapping positive integers to positive real numhat $f(n)$ is $Ω(g(n))$, pronounced “ $f(n)$ is big-Omega of $g(n)$,” if $g(n)$ is $O(f(n))$, that is, there is a real constant $c > 0$ and an integer constant $n_0 ≥ 1$ such that\n",
    "\\begin{gather*}\n",
    "f(n) \\ge cg(n),\\ for\\ n \\ge n_0.\n",
    "\\end{gather*}\n",
    "This definition allows us to say asymptotically that one function is greater than or equal to another, up to a constant factor.\n",
    "\n",
    "\n",
    "### Big-Theta\n",
    "\n",
    "This notation allows us to say that two functions grow at the same rate, up to constant factors. We say that $f(n)$ is $Θ(g(n))$, pronounced “ $f(n)$ is big-Theta of $g(n)$,” if $f(n)$ is $O(g(n))$ and $f(n)$ is $Ω(g(n))$ , that is, there are real constants $c^1 > 0$ and $c^n > 0$, and an integer constant n0 ≥ 1 such that\n",
    "\\begin{gather*}\n",
    "c^1g(n) \\le f(n) \\le c^ng(n),\\ for\\ n \\ge n_0.\n",
    "\\end{gather*}\n",
    "\n",
    "***\n",
    "\n",
    "## Comparative Analysis\n",
    "An asymptotically slow algorithm is beaten in the long run by an asymptotically faster algorithm, even if the constant factor for the asymptotically faster algorithm is worse.\n",
    "Table 3.3: Maximum size of a problem that can be solved in 1 second, 1 minute, and 1 hour, for various running times, measured in microseconds.\n",
    "\n",
    "\n",
    "**Some Words of Caution**\n",
    "\n",
    "* Even when using the big-Oh notation, we should at least be somewhat mindful of the constant factors and lower-order terms we are “hiding.”\n",
    "\n",
    "* Any algorithm running in $O(n\\ logn)$ time (with a reasonable constant factor) should be considered efficient. Even an $O(n^2)$-time function may be fast enough in some contexts, that is, when $n$ is small. But an algorithm running in $O(2^n)$ time should almost never be considered efficient.\n",
    "\n",
    "* If we must draw a line between efficient and inefficient algorithms, therefore, it is natural to make this distinction be that between those algorithms running in polynomial time and those running in exponential time.\n",
    "\n",
    "***\n",
    "\n",
    "## Examples of Algorithm Analysis\n",
    "\n",
    "**Constant-Time Operations**\n",
    "\n",
    "* The list class maintains, for each list, an instance variable that records the current length of the list. This allows it to immediately report that length, rather than take time to iteratively count each of the elements in the list.\n",
    "\n",
    "* The jth element of the list can be found, not by iterating through the list one element at a time, but by validating the index, and using it as an offset into the underlying array.\n",
    "\n",
    "* ```python\n",
    "def preﬁx average1(S):\n",
    "    ”””Return list such that, for all j, A[j] equals average of S[0], ..., S[j].”””\n",
    "    n = len(S)                          # create new list of n zeros\n",
    "    A = [0] * n\n",
    "    for j in range(n):\n",
    "        total = 0                       # begin computing S[0] + ... + S[j]\n",
    "        for i in range(j + 1):\n",
    "            total += S[i]\n",
    "        A[j] = total / (j+1)            # record the average\n",
    "    return A\n",
    "```\n",
    "    The statement, A = [0] * n, causes the creation and initialization of a Python list with length n, and with all entries equal to zero. This uses a constant number of primitive operations per element and thus runs in O(n) time.\n",
    "\n",
    "\n",
    "* ```python\n",
    "def preﬁx average2(S):\n",
    "    ”””Return list such that, for all j, A[j] equals average of S[0], ..., S[j].”””\n",
    "    n = len(S)                          # create new list of n zeros\n",
    "    A = [0] * n\n",
    "    for j in range(n):\n",
    "        A[j] = sum(S[0:j+1]) / (j+1)    # record the average\n",
    "    return A\n",
    "```\n",
    "    Even though the expression, sum(S[0:j+1]), seems like a single command, it is a function call and an evaluation of that function takes $O(j + 1)$ time in this context. Technically, the computation of the slice, S[0:j+1], also uses $O(j + 1)$ time, as it constructs a new list instance for storage.\n",
    "\n",
    "****\n",
    "\n",
    "**Three-Way Set Disjointness**\n",
    "\n",
    "Suppose we are given three sequences of numbers, $A, B,$ and $C$. We will assume that no individual sequence contains duplicate values, but that there may be some numbers that are in two or three of the sequences. The three-way set disjointness problem is to determine if the intersection of the three sequences is empty, namely, that there is no element $x$ such that $x \\in A, x \\in B$, and $x \\in C$.\n",
    "\n",
    "* A simple algorithm to solve this problem loops through each possible triple of values from the three sets to see if those values are equivalent. If each of the original sets has size $n$, then the worst-case running time of this function is $O(n^3)$.\n",
    "\n",
    "```python\n",
    "def disjoint1(A, B, C):\n",
    "    ”””Return True if there is no element common to all three lists.”””\n",
    "    for a in A:\n",
    "        for b in B:\n",
    "            for c in C:\n",
    "                if a == b == c:\n",
    "                    return False        # we found a common value\n",
    "    return True                         # if we reach this, sets are disjoint\n",
    "```\n",
    "\n",
    "* We can improve upon the asymptotic performance with a simple observation. Once inside the body of the loop over $B$, if selected elements a and b do not match each other, it is a waste of time to iterate through all values of $C$ looking for a matching triple. An improved solution, disjoint2 taking advantage of this observation is shown below, we claim that the worst-case running time for disjoint2 is $O(n^2)$. There are quadratically many pairs $(a, b)$ to consider. However, if $A$ and $B$ are each sets of distinct elements, there can be at most $O(n)$ such pairs with an equal to b. Therefore, the innermost loop, over $C$, executes at most $n$ times.\n",
    "\n",
    "```python\n",
    "def disjoint2(A, B, C):\n",
    "    ”””Return True if there is no element common to all three lists.”””\n",
    "    for a in A:\n",
    "        for b in B:\n",
    "            if a == b:                  # only check C if we found a match from A and B\n",
    "                for c in C:\n",
    "                    if a == c           # (and thus a == b == c)\n",
    "                        return False    # we found a common value\n",
    "    return True                         # if we reach this, sets are disjoint\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "**Element Uniqueness**\n",
    "\n",
    "In the element uniqueness problem, we are given a single sequence S with n elements and asked whether all elements of that collection are distinct from each other.\n",
    "\n",
    "* Our first solution to this problem uses a straightforward iterative algorithm. The unique1 function, given below, solves the element uniqueness problem by looping through all distinct pairs of indices $j < k$, checking if any of those pairs refer to elements that are equivalent to each other. Which we recognize as the familiar $O(n^2)$\n",
    "\n",
    "```python\n",
    "def unique1(S):\n",
    "    ”””Return True if there are no duplicate elements in sequence S.”””\n",
    "    for j in range(len(S)):\n",
    "        for k in range(j+1, len(S)):\n",
    "            if S[j] == S[k]:\n",
    "                return False            # found duplicate pair\n",
    "    return True                         # if we reach this, elements were unique\n",
    "```\n",
    "\n",
    "* **Using Sorting as a Problem-Solving Tool**: An even better algorithm for the element uniqueness problem is based on using sorting as a problem-solving tool. In this case, by sorting the sequence of elements, we are guaranteed that any duplicate elements will be placed next to each other. Thus, to determine if there are any duplicates, all we need to do is perform a single pass over the sorted sequence, looking for consecutive duplicates. A Python implementation of this algorithm  with a running time of $O(n\\ logn)$ is as follows:\n",
    "\n",
    "```python\n",
    "def unique2(S):\n",
    "    ”””Return True if there are no duplicate elements in sequence S.”””\n",
    "    temp = sorted(S)                   # create a sorted copy of S\n",
    "    for j in range(1, len(temp)):\n",
    "        if S[j−1] == S[j]:\n",
    "            return False               # found duplicate pair\n",
    "    return True                        # if we reach this, elements were unique\n",
    "```\n",
    "***\n",
    "***\n",
    "\n",
    "# Simple Justification Techniques\n",
    "\n",
    "## By Example\n",
    "Some claims are of the generic form, “There is an element x in a set $S$ that has\n",
    "property $P$.” To justify such a claim, we only need to produce a particular $x$ in $S$\n",
    "that has property $P$. Likewise, some hard-to-believe claims are of the generic form,\n",
    "“Every element $x$ in a set $S$ has property $P$.” To justify that such a claim is false, we\n",
    "only need to produce a particular $x$ from $S$ that does not have property $P$. Such an\n",
    "instance is called a ***counterexample***.\n",
    "***\n",
    "\n",
    "## The “Contra” Attack\n",
    "This technique involves the use of the negative. The two primary such methods are the use of the ***contrapositive*** and the ***contradiction***.\n",
    "\n",
    "* **Contrapositive**\\\n",
    "The use of the contrapositive method is like looking through a negative mirror. To justify the statement “if $p$ is true, then $q$ is true,” we establish that “if $q$ is not true, then $p$ is not true” instead.\n",
    "\n",
    "* **Contradiction**\\\n",
    "In applying the justification by contradiction technique, we establish that a statement $q$ is true by first supposing that $q$ is false and then showing that this assumption leads to a contradiction (such as $q$ is false and then showing that this assumption leads to a contradiction ($2 \\neq 2$ or $1 > 3$). By reaching such a contradiction, we show that no consistent situation exists with q being false, so q must be true. Of course, to reach this conclusion, we must be sure our situation is consistent before we assume q is false.\n",
    "***\n",
    "\n",
    "## Induction and Loop Invariants\n",
    "**Induction**\n",
    "\n",
    "* This technique amounts to showing that, for any particular $n ≥ 1$, there is a finite sequence of implications that starts with something known to be true and ultimately leads to showing that q(n) is true.\n",
    "\n",
    "* We should remember, however, the concreteness of the inductive technique. It shows that, for any particular n, there is a finite step-by-step sequence of implications that starts with something true and leads to the truth about n. In short, the inductive argument is a template for building a sequence of direct justifications.\n",
    "\n",
    "**Loop Invariants**\\\n",
    "To prove some statement $L$ about a loop is correct, define $L$ in terms of a series of smaller statements $L_0, L_1,..., L_k,$ where:\n",
    "\n",
    "1. The ***initial*** claim, $L_0$, is true before the loop begins.\n",
    "\n",
    "2. If $L_{j−1}$ is true before iteration $j$, then $L_j$ will be true after iteration $j$.\n",
    "\n",
    "3. The final statement, $L_k$, implies the desired statement $L$ to be true."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
